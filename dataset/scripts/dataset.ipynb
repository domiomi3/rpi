{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from ushuffle import shuffle\n",
    "from annotate.get_RNA_family import rfam_scan_single_sequence\n",
    "from annotate.cluster_sequences import get_new_sequences, get_new_sequences_protein\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Creation\n",
    "This dataset helps to create our final dataset with the given splits (Training Set, Test Set, Random Test Set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET_PATH = f\"../results/\"\n",
    "pathlib.Path(DATASET_PATH).mkdir(parents=True, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Join Sequences into DB"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load RAW RNAInter database\n",
    "rna_inter_df = utils.load_rna_inter_csv(path=\"../Download_data_RP.txt\")\n",
    "rna_sequences_families = pd.read_parquet(os.path.join(DATASET_PATH, 'rna_sequences_short_families.parquet'), engine='pyarrow')\n",
    "rna_sequences_clusters = pd.read_parquet(os.path.join(DATASET_PATH, '../results/rna_sequences_clusters.parquet'), engine='pyarrow')\n",
    "assert rna_sequences_families.shape[0] == rna_sequences_clusters.shape[0]\n",
    "rna_sequences = rna_sequences_clusters.merge(rna_sequences_families, on=['Raw_ID1', 'Sequence_1_ID', 'Sequence_1', 'Sequence_1_len', 'Sequence_1_shuffle'], how='inner')\n",
    "assert rna_sequences.shape[0] == rna_sequences_clusters.shape[0]\n",
    "protein_sequences = pd.read_parquet(os.path.join(DATASET_PATH, '../results/protein_sequences_clusters.parquet'), engine='pyarrow')\n",
    "all_interactions = rna_inter_df.merge(rna_sequences, on='Raw_ID1', how='inner').merge(protein_sequences, on='Raw_ID2', how='inner')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Removing uncertainty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# As there are many possible sequences for a single referenced ID in RNAInter\n",
    "# we only keep pairs with only one existing RNA and protein sequence.\n",
    "very_unique = all_interactions.groupby('RNAInterID').count().reset_index()\n",
    "very_unique = very_unique[very_unique['Category1'] == 1][['RNAInterID']]\n",
    "# Alternativly a random combination of avaialble sequences can be taken, e.g. \n",
    "# all_interactions.sample(frac=1) #  shuffles the dataframe randomly\n",
    "# all_interactions = all_interactions.drop_duplicates(subset=['RNAInterID']) #  keeps only the first appearance\n",
    "\n",
    "all_interactions = all_interactions.merge(very_unique, on='RNAInterID', how='inner')\n",
    "# We also remove interaction pairs where the RNA family is not known.\n",
    "all_interactions = all_interactions[all_interactions['Sequence_1_family'] != 'unknown']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Limit max amount of protein-Interactions per RNA\n",
    "limited_interactions = all_interactions.groupby(by=['Raw_ID1']).filter(lambda x: len(x) < 150)\n",
    "# limit max amount of rna-interactions per protein\n",
    "limited_interactions = limited_interactions.groupby(by=['Raw_ID2']).filter(lambda x: len(x) < 150)\n",
    "limited_interactions.to_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Creating Test Set based on RNA-family"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "limited_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')\n",
    "test_set_len = round(limited_interactions.shape[0] * 0.05)\n",
    "# pick random rna family\n",
    "train_set_interactions = limited_interactions\n",
    "rna_families = list(train_set_interactions['Sequence_1_family'].unique())\n",
    "#  build test set\n",
    "test_set_interactions = pd.DataFrame()\n",
    "while test_set_interactions.shape[0] < test_set_len:\n",
    "    rna_family = choice(rna_families)\n",
    "    temp_df = train_set_interactions[train_set_interactions['Sequence_1_family'] == rna_family]\n",
    "    # temp_proteins = list(temp_df['Raw_ID2'].unique())\n",
    "    #for protein in temp_proteins:\n",
    "    #    train_set_interactions = train_set_interactions.drop(train_set_interactions[train_set_interactions['Raw_ID2'] == protein].index)\n",
    "    if temp_df.shape[0] > test_set_len:\n",
    "        continue\n",
    "    rna_families.remove(rna_family)\n",
    "    train_set_interactions = train_set_interactions.drop(train_set_interactions[train_set_interactions['Sequence_1_family'] == rna_family].index)\n",
    "    test_set_interactions = pd.concat([test_set_interactions, temp_df])\n",
    "\n",
    "assert train_set_interactions.shape[0] + test_set_interactions.shape[0] == limited_interactions.shape[0]\n",
    "# build valid set\n",
    "print(f\"Size of train-set: {train_set_interactions.shape[0]} -- {round(train_set_interactions.shape[0] / limited_interactions.shape[0] * 100, 2)} %\")\n",
    "print(f\"Size of test-set: {test_set_interactions.shape[0]} -- {round(test_set_interactions.shape[0] / limited_interactions.shape[0] * 100, 2)} %\")\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'org_train_set.parquet'), engine='pyarrow')\n",
    "test_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'org_test_set.parquet'), engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This function SELECTS for each interaction a new RNA-interaction partner from the entire dataset for a given protein-interaction partner\n",
    "def increase_set_1(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Increase test-set for each protein-interaction\n",
    "    increased_set = pd.DataFrame()\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID2 = row['Raw_ID2']\n",
    "        rna_family = row['Sequence_1_family']\n",
    "        rna_cluster = row['Sequence_1_cluster']\n",
    "        rna_category = row['Category1']\n",
    "        # filter out every RNA that interacts with the same protein (Raw_ID2)\n",
    "        temp_df = limited_interactions[limited_interactions['Raw_ID2'] != Raw_ID2]\n",
    "        # Filter out same rna family of interactor\n",
    "        temp_df = temp_df[temp_df['Sequence_1_family'] != rna_family]\n",
    "        # Filter out same rna cluster of interactor\n",
    "        temp_df = temp_df[temp_df['Sequence_1_cluster'] != rna_cluster]\n",
    "        # Filter out same rna type of interactor\n",
    "        temp_df = temp_df[temp_df['Category1'] != rna_category]\n",
    "        assert temp_df.shape[0] != 0\n",
    "        while True:\n",
    "            random_row = temp_df.sample().to_dict('records')[0]\n",
    "            if dataset[\n",
    "                (dataset['Raw_ID2'] == row['Raw_ID2']) &\n",
    "                (dataset['Sequence_2_ID'] == row['Sequence_2_ID']) &\n",
    "                (dataset['Raw_ID1'] == random_row['Raw_ID1']) &\n",
    "                (dataset['Sequence_1_ID'] == random_row['Sequence_1_ID'])\n",
    "            ].shape[0] == 0:\n",
    "                break\n",
    "            # print(\"Oups, some duplicate found\")\n",
    "\n",
    "        # merge random row and row\n",
    "        # Remove RNA elements from row\n",
    "        row = row.to_dict()\n",
    "        for k in ('Raw_ID1', 'Interactor1.Symbol', 'Category1', 'Species1', 'Sequence_1', 'Sequence_1_len',\n",
    "           'Sequence_1_ID', 'Sequence_1_shuffle', 'Sequence_1_cluster',\n",
    "           'Sequence_1_cluster_sim', 'Sequence_1_cluster_reference',\n",
    "           'Sequence_1_rfam_q_accession', 'Sequence_1_family',\n",
    "           'Sequence_1_rfam_t_accession', 'Sequence_1_rfam_description',\n",
    "           'Sequence_1_rfam_e_value', 'Id'):\n",
    "            row.pop(k, None)\n",
    "        # Remove protein elements from new rom\n",
    "        for k in ('Raw_ID2','Interactor2.Symbol', 'Category2', 'Species2', 'Sequence_2_ID', 'Sequence_2',\n",
    "           'Sequence_2_len', 'Sequence_2_shuffle', 'Sequence_2_cluster', 'Sequence_2_cluster_sim', 'Sequence_2_cluster_reference',\n",
    "            'score', 'strong', 'weak', 'predict', 'RNAInterID', 'Id'):\n",
    "            random_row.pop(k, None)\n",
    "        new_row = {**row, **random_row, 'score': 0, 'Sequence_1_shuffle': True, 'strong': float(\"nan\"),\n",
    "                   'weak': float(\"nan\"), 'predict': float(\"nan\")}\n",
    "        new_row['RNAInterID'] = new_row['RNAInterID'] + '.RV1'\n",
    "        increased_set = pd.concat([increased_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    assert dataset.shape[0] == increased_set.shape[0]\n",
    "    assert dataset.merge(increased_set, on=['Raw_ID1', 'Sequence_1_ID', 'Raw_ID2', 'Sequence_2_ID'], how='inner').shape[0] == 0\n",
    "    return increased_set\n",
    "\n",
    "# This function SELECTS for each interaction a new protein-interaction partner from the entire dataset for a given rna-interaction partner\n",
    "def increase_set_2(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Increase test-set for each rna-interaction\n",
    "    increased_set = pd.DataFrame()\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID1 = row['Raw_ID1']\n",
    "        protein_cluster = row['Sequence_2_cluster']\n",
    "        protein_category = row['Category2']\n",
    "        # filter out every RNA that interacts with the same protein (Raw_ID2)\n",
    "        temp_df = limited_interactions[limited_interactions['Raw_ID1'] != Raw_ID1]\n",
    "        # Filter out same rna family of interactor\n",
    "        # Filter out same rna cluster of interactor\n",
    "        temp_df = temp_df[temp_df['Sequence_2_cluster'] != protein_cluster]\n",
    "        # Filter out same rna type of interactor\n",
    "        temp_df = temp_df[temp_df['Category2'] != protein_cluster]\n",
    "        assert temp_df.shape[0] != 0\n",
    "        while True:\n",
    "            random_row = temp_df.sample().to_dict('records')[0]\n",
    "            if dataset[\n",
    "                (dataset['Raw_ID1'] == row['Raw_ID1']) &\n",
    "                (dataset['Sequence_1_ID'] == row['Sequence_1_ID']) &\n",
    "                (dataset['Raw_ID2'] == random_row['Raw_ID2']) &\n",
    "                (dataset['Sequence_2_ID'] == random_row['Sequence_2_ID'])\n",
    "            ].shape[0] == 0:\n",
    "                break\n",
    "            # print(\"Oups, some duplicate found\")\n",
    "\n",
    "        # merge random row and row\n",
    "        # Remove protein elements from row\n",
    "        row = row.to_dict()\n",
    "        for k in ('Raw_ID2', 'Interactor2.Symbol', 'Category2', 'Species2', 'Sequence_2', 'Sequence_2_len',\n",
    "           'Sequence_2_ID', 'Sequence_2_shuffle', 'Sequence_2_cluster',\n",
    "           'Sequence_2_cluster_sim', 'Sequence_2_cluster_reference',\n",
    "            'Id'):\n",
    "            row.pop(k, None)\n",
    "        # Remove rna elements from new rom\n",
    "        for k in ('Raw_ID1', 'Interactor1.Symbol', 'Category1', 'Species1', 'Sequence_1', 'Sequence_1_len',\n",
    "           'Sequence_1_ID', 'Sequence_1_shuffle', 'Sequence_1_cluster',\n",
    "           'Sequence_1_cluster_sim', 'Sequence_1_cluster_reference',\n",
    "           'Sequence_1_rfam_q_accession', 'Sequence_1_family',\n",
    "           'Sequence_1_rfam_t_accession', 'Sequence_1_rfam_description',\n",
    "           'Sequence_1_rfam_e_value',\n",
    "            'score', 'strong', 'weak', 'predict', 'RNAInterID', 'Id'):\n",
    "            random_row.pop(k, None)\n",
    "        new_row = {**row, **random_row}\n",
    "        new_row['score'] = 0\n",
    "        new_row['Sequence_2_shuffle'] = True\n",
    "        new_row['strong'] = float(\"nan\")\n",
    "        new_row['weak'] = float(\"nan\")\n",
    "        new_row['predict'] = float(\"nan\")\n",
    "        new_row['RNAInterID'] = new_row['RNAInterID'] + '.PV1'\n",
    "        increased_set = pd.concat([increased_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    assert dataset.shape[0] == increased_set.shape[0]\n",
    "    assert dataset.merge(increased_set, on=['Raw_ID1', 'Sequence_1_ID', 'Raw_ID2', 'Sequence_2_ID'], how='inner').shape[0] == 0\n",
    "    return increased_set\n",
    "\n",
    "# Shuffle rna-sequence per protein-interaction\n",
    "def increase_set_3(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    increased_set = pd.DataFrame()\n",
    "    rna_clusters_all = pd.read_parquet('../results/rna_sequences_clusters.parquet', engine='pyarrow')\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID2 = row['Raw_ID2']\n",
    "        # get all families and clusters that interact with this protein.\n",
    "        # We want to prevent to generate any similar RNA\n",
    "        rna_families = set(limited_interactions[limited_interactions['Raw_ID2'] == Raw_ID2]['Sequence_1_family'])\n",
    "        rna_clusters = set(limited_interactions[limited_interactions['Raw_ID2'] == Raw_ID2]['Sequence_1_cluster'])\n",
    "        # Shuffle/Generate new RNA sequence\n",
    "        sequence = row['Sequence_1']\n",
    "        while True:\n",
    "            shuffled_seq = shuffle(sequence.encode('ASCII'), 2).decode('ASCII')\n",
    "            # assign rfam family to new sequence\n",
    "            new_row = row\n",
    "            row['Sequence_1'] = shuffled_seq\n",
    "            if rfam_scan_single_sequence(row, row['Sequence_1_family']):\n",
    "                continue\n",
    "            # assign cluster to new sequence\n",
    "            if get_new_sequences(rna_clusters_all[rna_clusters_all['Sequence_1_cluster'] == row['Sequence_1_cluster']],\n",
    "                                 row):\n",
    "                continue\n",
    "            new_row['RNAInterID'] = new_row['RNAInterID'] + \".RV2\"\n",
    "            new_row['Sequence_1_shuffle'] = True\n",
    "            new_row['score'] = 0\n",
    "            new_row['strong'] = float(\"nan\")\n",
    "            new_row['weak'] = float(\"nan\")\n",
    "            new_row['predict'] = float(\"nan\")\n",
    "            increased_set = pd.concat([increased_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            break\n",
    "    return increased_set\n",
    "\n",
    "# Shuffle protein-sequence per rna-interaction\n",
    "def increase_set_4(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    increased_set = pd.DataFrame()\n",
    "    protein_clusters_all = pd.read_parquet('../results/protein_sequences_clusters.parquet', engine='pyarrow')\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID1 = row['Raw_ID1']\n",
    "        # get all families and clusters that interact with this protein.\n",
    "        # We want to prevent to generate any similar RNA\n",
    "        protein_clusters = set(limited_interactions[limited_interactions['Raw_ID1'] == Raw_ID1]['Sequence_2_cluster'])\n",
    "        # Shuffle/Generate new Protein sequence\n",
    "        sequence = row['Sequence_2']\n",
    "        while True:\n",
    "            shuffled_seq = shuffle(sequence.encode('ASCII'), 2).decode('ASCII')\n",
    "            new_row = row\n",
    "            row['Sequence_2'] = shuffled_seq\n",
    "\n",
    "            # assign cluster to new sequence\n",
    "            if get_new_sequences_protein(protein_clusters_all[protein_clusters_all['Sequence_2_cluster'] == row['Sequence_2_cluster']],\n",
    "                                 row):\n",
    "                print(\".\")\n",
    "                continue\n",
    "            new_row['RNAInterID'] = new_row['RNAInterID'] + \".PV2\"\n",
    "            new_row['Sequence_2_shuffle'] = True\n",
    "            new_row['score'] = 0\n",
    "            new_row['strong'] = float(\"nan\")\n",
    "            new_row['weak'] = float(\"nan\")\n",
    "            new_row['predict'] = float(\"nan\")\n",
    "            increased_set = pd.concat([increased_set, pd.DataFrame([row])], ignore_index=True)\n",
    "            break\n",
    "    return increased_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Create negative interactions for the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'org_test_set.parquet'), engine='pyarrow')\n",
    "limited_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')\n",
    "print(\"Starting to build test set\")\n",
    "test_org_size = test_set_interactions.shape[0]\n",
    "test_set_org = test_set_interactions\n",
    "test_set_1 = increase_set_1(limited_interactions, test_set_org)\n",
    "assert test_set_1.shape[0] == test_set_org.shape[0]\n",
    "test_set_2 = increase_set_2(limited_interactions, test_set_org)\n",
    "assert test_set_2.shape[0] == test_set_org.shape[0]\n",
    "\n",
    "test_set_interactions = pd.concat([test_set_org,\n",
    "                                    test_set_1,\n",
    "                                    test_set_2,\n",
    "                                    ])\n",
    "assert test_set_interactions.shape[0] == test_org_size * 3\n",
    "test_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'test_set_interactions.parquet'), engine='pyarrow')\n",
    "print(\"test set done...\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Create negative interactions for the training set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'org_train_set.parquet'), engine='pyarrow')\n",
    "limited_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')\n",
    "print(\"Starting to build train set\")\n",
    "train_org_size = train_set_interactions.shape[0]\n",
    "train_set_org = train_set_interactions\n",
    "train_set_1 = increase_set_1(limited_interactions, train_set_org)\n",
    "assert train_set_1.shape[0] == train_set_org.shape[0]\n",
    "train_set_2 = increase_set_2(limited_interactions, train_set_org)\n",
    "assert train_set_2.shape[0] == train_set_org.shape[0]\n",
    "\n",
    "train_set_interactions = pd.concat([train_set_org,\n",
    "                                    train_set_1,\n",
    "                                    train_set_2,\n",
    "                                   ])\n",
    "assert train_set_interactions.shape[0] == train_org_size * 3\n",
    "test_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'), engine='pyarrow')\n",
    "print(\"train set done...\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Create Random Test Test based on the Training Set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read datasets\n",
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'))\n",
    "print(f\"Size of train: {len(train_set_interactions)}\")\n",
    "# do another data random data split based on random\n",
    "test_set_random_interactions = train_set_interactions.sample(frac=0.1)\n",
    "train_set_interactions = train_set_interactions.drop(test_set_random_interactions.index)\n",
    "print(f\"Size of train: {len(train_set_interactions)}\")\n",
    "print(f\"Size of test-random: {len(test_set_random_interactions)}\")\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'))\n",
    "test_set_random_interactions.to_parquet(os.path.join(DATASET_PATH, 'test_set_radnom_interactions.parquet'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Export all unique RNAs and proteins fr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding unique IDs to all proteins and RNAs\n",
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'), engine='pyarrow')\n",
    "test_set_random_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'test_set_radnom_interactions.parquet'), engine='pyarrow')\n",
    "test_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'test_set_interactions.parquet'), engine='pyarrow')\n",
    "# Add unique ID for each distinct rna\n",
    "all_df = pd.concat([train_set_interactions, test_set_random_interactions, test_set_interactions])\n",
    "all_df['Sequence_1_ID_Unique'] = all_df.groupby(['Sequence_1']).ngroup()\n",
    "all_df = all_df.drop_duplicates(subset=['Sequence_1_ID_Unique'])\n",
    "assert train_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on=['Sequence_1'], how='inner').shape[0] == train_set_interactions.shape[0]\n",
    "train_set_interactions = train_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on=['Sequence_1'], how='inner')\n",
    "assert test_set_random_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner').shape[0] == test_set_random_interactions.shape[0]\n",
    "test_set_random_interactions = test_set_random_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner')\n",
    "assert test_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner').shape[0] == test_set_interactions.shape[0]\n",
    "test_set_interactions = test_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner')\n",
    "\n",
    "# add unique protein id for each distinct protein id\n",
    "all_df = pd.concat([train_set_interactions, test_set_random_interactions, test_set_interactions])\n",
    "all_df['Sequence_2_ID_Unique'] = all_df.groupby(['Sequence_2']).ngroup()\n",
    "all_df = all_df.drop_duplicates(subset=['Sequence_2_ID_Unique'])\n",
    "assert train_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner').shape[0] == train_set_interactions.shape[0]\n",
    "train_set_interactions = train_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner')\n",
    "assert test_set_random_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner').shape[0] == test_set_random_interactions.shape[0]\n",
    "test_set_random_interactions = test_set_random_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner')\n",
    "assert test_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner').shape[0] == test_set_interactions.shape[0]\n",
    "test_set_interactions = test_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner')\n",
    "\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'final_train_set.parquet'), engine='pyarrow')\n",
    "test_set_random_interactions.to_parquet(os.path.join(DATASET_PATH, 'final_test_set_random.parquet'), engine='pyarrow')\n",
    "test_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'final_test_set.parquet'), engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_train_set.parquet'), engine='pyarrow')\n",
    "test_set_random_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_test_set_random.parquet'), engine='pyarrow')\n",
    "test_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_test_set.parquet'), engine='pyarrow')\n",
    "# get unique proteins and RNAs\n",
    "all_df = pd.concat([\n",
    "    train_set_interactions,\n",
    "    test_set_random_interactions,\n",
    "    test_set_interactions\n",
    "])\n",
    "unique_proteins = all_df[['Sequence_2_ID_Unique', 'Sequence_2']].drop_duplicates()\n",
    "unique_RNAs = all_df[['Sequence_1_ID_Unique', 'Sequence_1']].drop_duplicates()\n",
    "unique_proteins.to_parquet(os.path.join(DATASET_PATH, 'unique_proteins.parquet'), engine='pyarrow')\n",
    "unique_RNAs.to_parquet(os.path.join(DATASET_PATH, 'unique_RNAs.parquet'), engine='pyarrow')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: Create fully random dataset based on random\n",
    "Can be used for an ablation experiment to see if the model is still able to learn when presenting random training data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create dataset with random labels\n",
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_train_set.parquet'), engine='pyarrow')\n",
    "train_set_interactions['Sequence_1_shuffle'] = 0\n",
    "train_set_interactions['Sequence_2_shuffle'] = 0\n",
    "assert len(train_set_interactions[(train_set_interactions['Sequence_1_shuffle'] == 1) | (train_set_interactions['Sequence_2_shuffle'] == 1)]) == 0\n",
    "train_set_interactions['Sequence_1_shuffle'] = np.random.choice([0, 1], train_set_interactions.shape[0], p=[1/3, 2/3])\n",
    "print(len(train_set_interactions[(train_set_interactions['Sequence_1_shuffle'] == 1) | (train_set_interactions['Sequence_2_shuffle'] == 1)]) / len(train_set_interactions))\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'random_train_set.parquet'))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "import numpy as np\n",
    "from random import choice\n",
    "from ushuffle import shuffle\n",
    "from annotate.get_RNA_family import rfam_scan_single_sequence\n",
    "from annotate.cluster_sequences import get_new_sequences, get_new_sequences_protein\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataset Creation\n",
    "This dataset helps to create our final dataset with the given splits (Training Set, Test Set, Random Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = f\"/work/dlclarge1/matusd-rpi/RPI/dataset/scripts/annotate/dataset/results/\"\n",
    "\n",
    "pathlib.Path(DATASET_PATH).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/work/dlclarge1/matusd-rpi/RPI/dataset/scripts/annotate/dataset/results/rna_sequences_short_families.parquet\n"
     ]
    }
   ],
   "source": [
    "a = os.path.join(DATASET_PATH, 'rna_sequences_short_families.parquet')\n",
    "print(os.path.exists(a))\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 1: Join Sequences into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load RAW RNAInter database\n",
    "rna_inter_df = utils.load_rna_inter_csv(path=\"/work/dlclarge1/matusd-rpi/RPI/Download_data_RP.txt\")\n",
    "rna_sequences_families = pd.read_parquet(os.path.join(DATASET_PATH, 'rna_sequences_short_families.parquet'), engine='pyarrow')\n",
    "rna_sequences_clusters = pd.read_parquet(os.path.join(DATASET_PATH, 'rna_sequences_clusters.parquet'), engine='pyarrow')\n",
    "assert rna_sequences_families.shape[0] == rna_sequences_clusters.shape[0]\n",
    "rna_sequences = rna_sequences_clusters.merge(rna_sequences_families, on=['Raw_ID1', 'Sequence_1_ID', 'Sequence_1', 'Sequence_1_len', 'Sequence_1_shuffle'], how='inner')\n",
    "assert rna_sequences.shape[0] == rna_sequences_clusters.shape[0]\n",
    "protein_sequences = pd.read_parquet(os.path.join(DATASET_PATH, 'protein_sequences_clusters.parquet'), engine='pyarrow')\n",
    "all_interactions = rna_inter_df.merge(rna_sequences, on='Raw_ID1', how='inner').merge(protein_sequences, on='Raw_ID2', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4908248, 33)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_interactions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 2: Removing uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All interactions: 153395\n"
     ]
    }
   ],
   "source": [
    "# As there are many possible sequences for a single referenced ID in RNAInter\n",
    "# we only keep pairs with only one existing RNA and protein sequence.\n",
    "very_unique = all_interactions.groupby('RNAInterID').count().reset_index()\n",
    "very_unique = very_unique[very_unique['Category1'] == 1][['RNAInterID']]\n",
    "# Alternativly a random combination of avaialble sequences can be taken, e.g. \n",
    "# all_interactions.sample(frac=1) #  shuffles the dataframe randomly\n",
    "# all_interactions = all_interactions.drop_duplicates(subset=['RNAInterID']) #  keeps only the first appearance\n",
    "\n",
    "all_interactions = all_interactions.merge(very_unique, on='RNAInterID', how='inner')\n",
    "# We also remove interaction pairs where the RNA family is not known.\n",
    "all_interactions = all_interactions[all_interactions['Sequence_1_family'] != 'unknown']\n",
    "print(f\"All interactions: {all_interactions.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limited interactions to:  15303\n"
     ]
    }
   ],
   "source": [
    "# Limit max amount of protein-Interactions per RNA\n",
    "limited_interactions = all_interactions.groupby(by=['Raw_ID1']).filter(lambda x: len(x) < 150)\n",
    "# limit max amount of rna-interactions per protein\n",
    "limited_interactions = limited_interactions.groupby(by=['Raw_ID2']).filter(lambda x: len(x) < 150)\n",
    "print(\"Limited interactions to: \", limited_interactions.shape[0])\n",
    "limited_interactions.to_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 3: Creating Test Set based on RNA-family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train-set: 14527 -- 94.93 %\n",
      "Size of test-set: 776 -- 5.07 %\n"
     ]
    }
   ],
   "source": [
    "limited_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')\n",
    "test_set_len = round(limited_interactions.shape[0] * 0.05)\n",
    "# pick random rna family\n",
    "train_set_interactions = limited_interactions\n",
    "rna_families = list(train_set_interactions['Sequence_1_family'].unique())\n",
    "#  build test set\n",
    "test_set_interactions = pd.DataFrame()\n",
    "while test_set_interactions.shape[0] < test_set_len:\n",
    "    rna_family = choice(rna_families)\n",
    "    temp_df = train_set_interactions[train_set_interactions['Sequence_1_family'] == rna_family]\n",
    "    # temp_proteins = list(temp_df['Raw_ID2'].unique())\n",
    "    #for protein in temp_proteins:\n",
    "    #    train_set_interactions = train_set_interactions.drop(train_set_interactions[train_set_interactions['Raw_ID2'] == protein].index)\n",
    "    if temp_df.shape[0] > test_set_len:\n",
    "        continue\n",
    "    rna_families.remove(rna_family)\n",
    "    train_set_interactions = train_set_interactions.drop(train_set_interactions[train_set_interactions['Sequence_1_family'] == rna_family].index)\n",
    "    test_set_interactions = pd.concat([test_set_interactions, temp_df])\n",
    "\n",
    "assert train_set_interactions.shape[0] + test_set_interactions.shape[0] == limited_interactions.shape[0]\n",
    "# build valid set\n",
    "print(f\"Size of train-set: {train_set_interactions.shape[0]} -- {round(train_set_interactions.shape[0] / limited_interactions.shape[0] * 100, 2)} %\")\n",
    "print(f\"Size of test-set: {test_set_interactions.shape[0]} -- {round(test_set_interactions.shape[0] / limited_interactions.shape[0] * 100, 2)} %\")\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'org_train_set.parquet'), engine='pyarrow')\n",
    "test_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'org_test_set.parquet'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function SELECTS for each interaction a new RNA-interaction partner from the entire dataset for a given protein-interaction partner\n",
    "def increase_set_1(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Increase test-set for each protein-interaction\n",
    "    increased_set = pd.DataFrame()\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID2 = row['Raw_ID2']\n",
    "        rna_family = row['Sequence_1_family']\n",
    "        rna_cluster = row['Sequence_1_cluster']\n",
    "        rna_category = row['Category1']\n",
    "        # filter out every RNA that interacts with the same protein (Raw_ID2)\n",
    "        temp_df = limited_interactions[limited_interactions['Raw_ID2'] != Raw_ID2]\n",
    "        # Filter out same rna family of interactor\n",
    "        temp_df = temp_df[temp_df['Sequence_1_family'] != rna_family]\n",
    "        # Filter out same rna cluster of interactor\n",
    "        temp_df = temp_df[temp_df['Sequence_1_cluster'] != rna_cluster]\n",
    "        # Filter out same rna type of interactor\n",
    "        temp_df = temp_df[temp_df['Category1'] != rna_category]\n",
    "        assert temp_df.shape[0] != 0\n",
    "        while True:\n",
    "            random_row = temp_df.sample().to_dict('records')[0]\n",
    "            if dataset[\n",
    "                (dataset['Raw_ID2'] == row['Raw_ID2']) &\n",
    "                (dataset['Sequence_2_ID'] == row['Sequence_2_ID']) &\n",
    "                (dataset['Raw_ID1'] == random_row['Raw_ID1']) &\n",
    "                (dataset['Sequence_1_ID'] == random_row['Sequence_1_ID'])\n",
    "            ].shape[0] == 0:\n",
    "                break\n",
    "            # print(\"Oups, some duplicate found\")\n",
    "\n",
    "        # merge random row and row\n",
    "        # Remove RNA elements from row\n",
    "        row = row.to_dict()\n",
    "        for k in ('Raw_ID1', 'Interactor1.Symbol', 'Category1', 'Species1', 'Sequence_1', 'Sequence_1_len',\n",
    "           'Sequence_1_ID', 'Sequence_1_shuffle', 'Sequence_1_cluster',\n",
    "           'Sequence_1_cluster_sim', 'Sequence_1_cluster_reference',\n",
    "           'Sequence_1_rfam_q_accession', 'Sequence_1_family',\n",
    "           'Sequence_1_rfam_t_accession', 'Sequence_1_rfam_description',\n",
    "           'Sequence_1_rfam_e_value', 'Id'):\n",
    "            row.pop(k, None)\n",
    "        # Remove protein elements from new rom\n",
    "        for k in ('Raw_ID2','Interactor2.Symbol', 'Category2', 'Species2', 'Sequence_2_ID', 'Sequence_2',\n",
    "           'Sequence_2_len', 'Sequence_2_shuffle', 'Sequence_2_cluster', 'Sequence_2_cluster_sim', 'Sequence_2_cluster_reference',\n",
    "            'score', 'strong', 'weak', 'predict', 'RNAInterID', 'Id'):\n",
    "            random_row.pop(k, None)\n",
    "        new_row = {**row, **random_row, 'score': 0, 'Sequence_1_shuffle': True, 'strong': float(\"nan\"),\n",
    "                   'weak': float(\"nan\"), 'predict': float(\"nan\")}\n",
    "        new_row['RNAInterID'] = new_row['RNAInterID'] + '.RV1'\n",
    "        increased_set = pd.concat([increased_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    assert dataset.shape[0] == increased_set.shape[0]\n",
    "    assert dataset.merge(increased_set, on=['Raw_ID1', 'Sequence_1_ID', 'Raw_ID2', 'Sequence_2_ID'], how='inner').shape[0] == 0\n",
    "    print(increased_set.shape[0])\n",
    "    return increased_set\n",
    "\n",
    "# This function SELECTS for each interaction a new protein-interaction partner from the entire dataset for a given rna-interaction partner\n",
    "def increase_set_2(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Increase test-set for each rna-interaction\n",
    "    increased_set = pd.DataFrame()\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID1 = row['Raw_ID1']\n",
    "        protein_cluster = row['Sequence_2_cluster']\n",
    "        protein_category = row['Category2']\n",
    "        # filter out every RNA that interacts with the same protein (Raw_ID2)\n",
    "        temp_df = limited_interactions[limited_interactions['Raw_ID1'] != Raw_ID1]\n",
    "        # Filter out same rna family of interactor\n",
    "        # Filter out same rna cluster of interactor\n",
    "        temp_df = temp_df[temp_df['Sequence_2_cluster'] != protein_cluster]\n",
    "        # Filter out same rna type of interactor\n",
    "        temp_df = temp_df[temp_df['Category2'] != protein_cluster]\n",
    "        assert temp_df.shape[0] != 0\n",
    "        while True:\n",
    "            random_row = temp_df.sample().to_dict('records')[0]\n",
    "            if dataset[\n",
    "                (dataset['Raw_ID1'] == row['Raw_ID1']) &\n",
    "                (dataset['Sequence_1_ID'] == row['Sequence_1_ID']) &\n",
    "                (dataset['Raw_ID2'] == random_row['Raw_ID2']) &\n",
    "                (dataset['Sequence_2_ID'] == random_row['Sequence_2_ID'])\n",
    "            ].shape[0] == 0:\n",
    "                break\n",
    "            # print(\"Oups, some duplicate found\")\n",
    "\n",
    "        # merge random row and row\n",
    "        # Remove protein elements from row\n",
    "        row = row.to_dict()\n",
    "        for k in ('Raw_ID2', 'Interactor2.Symbol', 'Category2', 'Species2', 'Sequence_2', 'Sequence_2_len',\n",
    "           'Sequence_2_ID', 'Sequence_2_shuffle', 'Sequence_2_cluster',\n",
    "           'Sequence_2_cluster_sim', 'Sequence_2_cluster_reference',\n",
    "            'Id'):\n",
    "            row.pop(k, None)\n",
    "        # Remove rna elements from new rom\n",
    "        for k in ('Raw_ID1', 'Interactor1.Symbol', 'Category1', 'Species1', 'Sequence_1', 'Sequence_1_len',\n",
    "           'Sequence_1_ID', 'Sequence_1_shuffle', 'Sequence_1_cluster',\n",
    "           'Sequence_1_cluster_sim', 'Sequence_1_cluster_reference',\n",
    "           'Sequence_1_rfam_q_accession', 'Sequence_1_family',\n",
    "           'Sequence_1_rfam_t_accession', 'Sequence_1_rfam_description',\n",
    "           'Sequence_1_rfam_e_value',\n",
    "            'score', 'strong', 'weak', 'predict', 'RNAInterID', 'Id'):\n",
    "            random_row.pop(k, None)\n",
    "        new_row = {**row, **random_row}\n",
    "        new_row['score'] = 0\n",
    "        new_row['Sequence_2_shuffle'] = True\n",
    "        new_row['strong'] = float(\"nan\")\n",
    "        new_row['weak'] = float(\"nan\")\n",
    "        new_row['predict'] = float(\"nan\")\n",
    "        new_row['RNAInterID'] = new_row['RNAInterID'] + '.PV1'\n",
    "        increased_set = pd.concat([increased_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    assert dataset.shape[0] == increased_set.shape[0]\n",
    "    assert dataset.merge(increased_set, on=['Raw_ID1', 'Sequence_1_ID', 'Raw_ID2', 'Sequence_2_ID'], how='inner').shape[0] == 0\n",
    "    print(increased_set.shape[0])\n",
    "    return increased_set\n",
    "\n",
    "# Shuffle rna-sequence per protein-interaction\n",
    "def increase_set_3(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    increased_set = pd.DataFrame()\n",
    "    rna_clusters_all = pd.read_parquet('rna_sequences_clusters.parquet', engine='pyarrow')\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID2 = row['Raw_ID2']\n",
    "        # get all families and clusters that interact with this protein.\n",
    "        # We want to prevent to generate any similar RNA\n",
    "        rna_families = set(limited_interactions[limited_interactions['Raw_ID2'] == Raw_ID2]['Sequence_1_family'])\n",
    "        rna_clusters = set(limited_interactions[limited_interactions['Raw_ID2'] == Raw_ID2]['Sequence_1_cluster'])\n",
    "        # Shuffle/Generate new RNA sequence\n",
    "        sequence = row['Sequence_1']\n",
    "        while True:\n",
    "            shuffled_seq = shuffle(sequence.encode('ASCII'), 2).decode('ASCII')\n",
    "            # assign rfam family to new sequence\n",
    "            new_row = row\n",
    "            row['Sequence_1'] = shuffled_seq\n",
    "            if rfam_scan_single_sequence(row, row['Sequence_1_family']):\n",
    "                continue\n",
    "            # assign cluster to new sequence\n",
    "            if get_new_sequences(rna_clusters_all[rna_clusters_all['Sequence_1_cluster'] == row['Sequence_1_cluster']],\n",
    "                                 row):\n",
    "                continue\n",
    "            new_row['RNAInterID'] = new_row['RNAInterID'] + \".RV2\"\n",
    "            new_row['Sequence_1_shuffle'] = True\n",
    "            new_row['score'] = 0\n",
    "            new_row['strong'] = float(\"nan\")\n",
    "            new_row['weak'] = float(\"nan\")\n",
    "            new_row['predict'] = float(\"nan\")\n",
    "            increased_set = pd.concat([increased_set, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            break\n",
    "    return increased_set\n",
    "\n",
    "# Shuffle protein-sequence per rna-interaction\n",
    "def increase_set_4(limited_interactions: pd.DataFrame, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    increased_set = pd.DataFrame()\n",
    "    protein_clusters_all = pd.read_parquet('protein_sequences_clusters.parquet', engine='pyarrow')\n",
    "    for _, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "        Raw_ID1 = row['Raw_ID1']\n",
    "        # get all families and clusters that interact with this protein.\n",
    "        # We want to prevent to generate any similar RNA\n",
    "        protein_clusters = set(limited_interactions[limited_interactions['Raw_ID1'] == Raw_ID1]['Sequence_2_cluster'])\n",
    "        # Shuffle/Generate new Protein sequence\n",
    "        sequence = row['Sequence_2']\n",
    "        while True:\n",
    "            shuffled_seq = shuffle(sequence.encode('ASCII'), 2).decode('ASCII')\n",
    "            new_row = row\n",
    "            row['Sequence_2'] = shuffled_seq\n",
    "\n",
    "            # assign cluster to new sequence\n",
    "            if get_new_sequences_protein(protein_clusters_all[protein_clusters_all['Sequence_2_cluster'] == row['Sequence_2_cluster']],\n",
    "                                 row):\n",
    "                print(\".\")\n",
    "                continue\n",
    "            new_row['RNAInterID'] = new_row['RNAInterID'] + \".PV2\"\n",
    "            new_row['Sequence_2_shuffle'] = True\n",
    "            new_row['score'] = 0\n",
    "            new_row['strong'] = float(\"nan\")\n",
    "            new_row['weak'] = float(\"nan\")\n",
    "            new_row['predict'] = float(\"nan\")\n",
    "            increased_set = pd.concat([increased_set, pd.DataFrame([row])], ignore_index=True)\n",
    "            break\n",
    "    return increased_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 4: Create negative interactions for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build test set\n",
      "Test set size: 776\n",
      "Limited interactions size: 15303\n",
      "Test org size: 776\n",
      "Test set org size: 776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 40/776 [00:00<00:16, 45.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 776/776 [00:16<00:00, 47.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n",
      "Test set 1 size: 776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 776/776 [00:14<00:00, 54.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "776\n",
      "Test set 2 size: 776\n",
      "Test set size: 2328\n",
      "test set done...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'org_test_set.parquet'), engine='pyarrow')\n",
    "limited_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')\n",
    "print(\"Starting to build test set\")\n",
    "print(f\"Test set size: {test_set_interactions.shape[0]}\")\n",
    "print(f\"Limited interactions size: {limited_interactions.shape[0]}\")\n",
    "test_org_size = test_set_interactions.shape[0]\n",
    "test_set_org = test_set_interactions\n",
    "print(f\"Test org size: {test_org_size}\")\n",
    "print(f\"Test set org size: {test_set_org.shape[0]}\")\n",
    "test_set_1 = increase_set_1(limited_interactions, test_set_org)\n",
    "print(f\"Test set 1 size: {test_set_1.shape[0]}\")\n",
    "assert test_set_1.shape[0] == test_set_org.shape[0]\n",
    "test_set_2 = increase_set_2(limited_interactions, test_set_org)\n",
    "print(f\"Test set 2 size: {test_set_2.shape[0]}\")\n",
    "assert test_set_2.shape[0] == test_set_org.shape[0]\n",
    "\n",
    "test_set_interactions = pd.concat([test_set_org,\n",
    "                                    test_set_1,\n",
    "                                    test_set_2,\n",
    "                                    ])\n",
    "print(f\"Test set size: {test_set_interactions.shape[0]}\")\n",
    "assert test_set_interactions.shape[0] == test_org_size * 3\n",
    "test_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'test_set_interactions.parquet'), engine='pyarrow')\n",
    "print(\"test set done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence_1_shuffle\n",
      "False    1552\n",
      "True      776\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    1552\n",
      "True      776\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "True    776\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    776\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "False    776\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "True    776\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "False    776\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    776\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(test_set_interactions['Sequence_1_shuffle'].value_counts())\n",
    "print(test_set_interactions['Sequence_2_shuffle'].value_counts())\n",
    "print(test_set_1['Sequence_1_shuffle'].value_counts())\n",
    "print(test_set_1['Sequence_2_shuffle'].value_counts())\n",
    "print(test_set_2['Sequence_1_shuffle'].value_counts())\n",
    "print(test_set_2['Sequence_2_shuffle'].value_counts())\n",
    "print(test_set_org['Sequence_1_shuffle'].value_counts())\n",
    "print(test_set_org['Sequence_2_shuffle'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 5: Create negative interactions for the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build train set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/14527 [00:00<05:48, 41.66it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14527/14527 [06:11<00:00, 39.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14527/14527 [05:32<00:00, 43.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14527\n",
      "train set done...\n"
     ]
    }
   ],
   "source": [
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'org_train_set.parquet'), engine='pyarrow')\n",
    "limited_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'limited_interactions.parquet'), engine='pyarrow')\n",
    "print(\"Starting to build train set\")\n",
    "train_org_size = train_set_interactions.shape[0]\n",
    "train_set_org = train_set_interactions\n",
    "train_set_1 = increase_set_1(limited_interactions, train_set_org)\n",
    "assert train_set_1.shape[0] == train_set_org.shape[0]\n",
    "train_set_2 = increase_set_2(limited_interactions, train_set_org)\n",
    "assert train_set_2.shape[0] == train_set_org.shape[0]\n",
    "\n",
    "train_set_interactions = pd.concat([train_set_org,\n",
    "                                    train_set_1,\n",
    "                                    train_set_2,\n",
    "                                   ])\n",
    "assert train_set_interactions.shape[0] == train_org_size * 3\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'), engine='pyarrow')\n",
    "print(\"train set done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence_1_shuffle\n",
      "False    29054\n",
      "True     14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    29054\n",
      "True     14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "True    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "True    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set_interactions['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_interactions['Sequence_2_shuffle'].value_counts())\n",
    "print(train_set_1['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_1['Sequence_2_shuffle'].value_counts())\n",
    "print(train_set_2['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_2['Sequence_2_shuffle'].value_counts())\n",
    "print(train_set_org['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_org['Sequence_2_shuffle'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 6: Create Random Test Test based on the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train: 43581\n",
      "Size of train: 36532\n",
      "Size of test-random: 4358\n"
     ]
    }
   ],
   "source": [
    "# read datasets\n",
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'))\n",
    "print(f\"Size of train: {len(train_set_interactions)}\")\n",
    "# do another data random data split based on random\n",
    "test_set_random_interactions = train_set_interactions.sample(frac=0.1)\n",
    "train_set_interactions = train_set_interactions.drop(test_set_random_interactions.index)\n",
    "print(f\"Size of train: {len(train_set_interactions)}\")\n",
    "print(f\"Size of test-random: {len(test_set_random_interactions)}\")\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'))\n",
    "test_set_random_interactions.to_parquet(os.path.join(DATASET_PATH, 'test_set_radnom_interactions.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Step 7: Export all unique RNAs and proteins fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding unique IDs to all proteins and RNAs\n",
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'train_set_interactions.parquet'), engine='pyarrow')\n",
    "test_set_random_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'test_set_radnom_interactions.parquet'), engine='pyarrow')\n",
    "test_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'test_set_interactions.parquet'), engine='pyarrow')\n",
    "# Add unique ID for each distinct rna\n",
    "all_df = pd.concat([train_set_interactions, test_set_random_interactions, test_set_interactions])\n",
    "all_df['Sequence_1_ID_Unique'] = all_df.groupby(['Sequence_1']).ngroup()\n",
    "all_df = all_df.drop_duplicates(subset=['Sequence_1_ID_Unique'])\n",
    "assert train_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on=['Sequence_1'], how='inner').shape[0] == train_set_interactions.shape[0]\n",
    "train_set_interactions = train_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on=['Sequence_1'], how='inner')\n",
    "assert test_set_random_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner').shape[0] == test_set_random_interactions.shape[0]\n",
    "test_set_random_interactions = test_set_random_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner')\n",
    "assert test_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner').shape[0] == test_set_interactions.shape[0]\n",
    "test_set_interactions = test_set_interactions.merge(all_df[['Sequence_1_ID_Unique', 'Sequence_1']], on='Sequence_1', how='inner')\n",
    "\n",
    "# add unique protein id for each distinct protein id\n",
    "all_df = pd.concat([train_set_interactions, test_set_random_interactions, test_set_interactions])\n",
    "all_df['Sequence_2_ID_Unique'] = all_df.groupby(['Sequence_2']).ngroup()\n",
    "all_df = all_df.drop_duplicates(subset=['Sequence_2_ID_Unique'])\n",
    "assert train_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner').shape[0] == train_set_interactions.shape[0]\n",
    "train_set_interactions = train_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner')\n",
    "assert test_set_random_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner').shape[0] == test_set_random_interactions.shape[0]\n",
    "test_set_random_interactions = test_set_random_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner')\n",
    "assert test_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner').shape[0] == test_set_interactions.shape[0]\n",
    "test_set_interactions = test_set_interactions.merge(all_df[['Sequence_2_ID_Unique', 'Sequence_2']], on='Sequence_2', how='inner')\n",
    "\n",
    "train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'final_train_set.parquet'), engine='pyarrow')\n",
    "test_set_random_interactions.to_parquet(os.path.join(DATASET_PATH, 'final_test_set_random.parquet'), engine='pyarrow')\n",
    "test_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'final_test_set.parquet'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_train_set.parquet'), engine='pyarrow')\n",
    "test_set_random_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_test_set_random.parquet'), engine='pyarrow')\n",
    "test_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_test_set.parquet'), engine='pyarrow')\n",
    "# get unique proteins and RNAs\n",
    "all_df = pd.concat([\n",
    "    train_set_interactions,\n",
    "    test_set_random_interactions,\n",
    "    test_set_interactions\n",
    "])\n",
    "unique_proteins = all_df[['Sequence_2_ID_Unique', 'Sequence_2']].drop_duplicates()\n",
    "unique_RNAs = all_df[['Sequence_1_ID_Unique', 'Sequence_1']].drop_duplicates()\n",
    "unique_proteins.to_parquet(os.path.join(DATASET_PATH, 'unique_proteins.parquet'), engine='pyarrow')\n",
    "unique_RNAs.to_parquet(os.path.join(DATASET_PATH, 'unique_RNAs.parquet'), engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence_1_shuffle\n",
      "False    24806\n",
      "True     11726\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    24806\n",
      "True     11726\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "True    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "True    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_1_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n",
      "Sequence_2_shuffle\n",
      "False    14527\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_set_interactions['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_interactions['Sequence_2_shuffle'].value_counts())\n",
    "print(train_set_1['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_1['Sequence_2_shuffle'].value_counts())\n",
    "print(train_set_2['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_2['Sequence_2_shuffle'].value_counts())\n",
    "print(train_set_org['Sequence_1_shuffle'].value_counts())\n",
    "print(train_set_org['Sequence_2_shuffle'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique proteins: 656\n",
      "Unique RNAs: 3008\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique proteins: {unique_proteins.shape[0]}\")\n",
    "print(f\"Unique RNAs: {unique_RNAs.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optional: Create fully random dataset based on random\n",
    "Can be used for an ablation experiment to see if the model is still able to learn when presenting random training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create dataset with random labels\n",
    "# train_set_interactions = pd.read_parquet(os.path.join(DATASET_PATH, 'final_train_set.parquet'), engine='pyarrow')\n",
    "# train_set_interactions['Sequence_1_shuffle'] = 0\n",
    "# train_set_interactions['Sequence_2_shuffle'] = 0\n",
    "# assert len(train_set_interactions[(train_set_interactions['Sequence_1_shuffle'] == 1) | (train_set_interactions['Sequence_2_shuffle'] == 1)]) == 0\n",
    "# train_set_interactions['Sequence_1_shuffle'] = np.random.choice([0, 1], train_set_interactions.shape[0], p=[1/3, 2/3])\n",
    "# print(len(train_set_interactions[(train_set_interactions['Sequence_1_shuffle'] == 1) | (train_set_interactions['Sequence_2_shuffle'] == 1)]) / len(train_set_interactions))\n",
    "# train_set_interactions.to_parquet(os.path.join(DATASET_PATH, 'random_train_set.parquet'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
